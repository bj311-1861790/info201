select(-occupation)
dim(affairs_clean)
# Inspect first few rows
head(affairs_clean)
# Inspect last few rows
tail(affairs_clean)
summary(affairs_clean)
food_atlas <- read.csv(file.choose(food_atlas.csv), header = T, sep = ',')
# 2
food_atlas <- read.csv(file.choose(./food_atlas.csv), header = T, sep = ',')
food_atlas <- read.csv(file.choose(food_atlas.csv), header = T, sep = ',')
# 2
food_atlas <- read.csv(file.choose(food_atlas.csv), header = T, sep = ',')
setwd("C:/Users/33786/OneDrive/info201")
# 2
food_atlas <- read.csv(file.choose(food_atlas.csv), header = T, sep = ',')
setwd("~/")
View(snap)
View(snap)
# 2
food_atlas <- read.csv(file.choose(), header = T, sep = ',')
View(food_atlas)
View(snap)
snap <- read.csv("food_atlas.csv", stringsAsFactors = FALSE)
View(snap)
food_atlas <- read.csv(file.choose(), header = T, sep = ',')
# 3
snap <- read.csv("food_atlas.csv", stringsAsFactors = FALSE)
View(snap)
# 2
food_atlas <- read.csv(file.choose(), header = T, sep = ',')
# 3
snap <- read.csv("food_atlas.csv", stringsAsFactors = FALSE)
View(snap)
#4
nap <- read.csv("food_atlas.csv", stringsAsFactors = FALSE)
View(snap)
#There are 3143 observations and 20 variables.
#4
head(snap)
#5
tail(snap)
# 3
snap <- read.csv("./food_atlas.csv", stringsAsFactors = FALSE)
# 3
snap <- read.csv("food_atlas.csv", stringsAsFactors = FALSE)
# 3
snap <- read.csv("food_atlas", stringsAsFactors = FALSE)
food_atlas <- read.csv(file.choose(), header = T, sep = ',')
# 3
snap <- read.csv("food_atlas.csv", stringsAsFactors = FALSE)
View(snap)
food_atlas <- read.csv(file.choose(), header = T, sep = ',')
# 3
View(food_atlas)
#There are 3143 observations and 20 variables.
#4
head(food_atlas)
#5
tail(food_atlas)
#6
summary(food_atlas)
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
library(oilabs)
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
library(oilabs)
## Set a seed here
coin_outcomes <- c(0,1)
simulation_results <- sample(coin_outcomes, size=50, replace=TRUE)
simulation_results %>% sum()
simulation_results %>% mean()
## Set a seed here
coin_outcomes <- c(0,1)
simulation_results <- sample(coin_outcomes, size=50, replace=TRUE)
simulation_results %>% sum()
simulation_results %>% mean()
## Set a seed here
coin_outcomes <- c(0,1)
simulation_results <- sample(coin_outcomes, size=50, replace=TRUE)
simulation_results %>% sum()
simulation_results %>% mean()
## Set a seed here
set.seed(123)
coin_outcomes <- c(0,1)
simulation_results <- sample(coin_outcomes, size=50, replace=TRUE)
simulation_results %>% sum()
simulation_results %>% mean()
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
library(oilabs)
set.seed(5)
coin_outcomes <- c(0,1)
simulation_results <- sample(coin_outcomes, size=50, replace=TRUE)
simulation_results %>% sum()
simulation_results %>% mean()
set.seed(311)
tosses = sample(coin_outcomes, size = 10000, replace=TRUE)
sum_so_far = cumsum(tosses)
sum_so_far[277]
mean_so_far = sum_so_far/(1:10000)
mean_so_far[112]
coin_tosses <- data.frame(n=1:10000, expected_sum_so_far = 1:10000/2,
sum_so_far = sum_so_far,
mean_so_far = mean_so_far,
expected_mean_so_far = 0.5)
head(coin_tosses)
ggplot(data = coin_tosses, aes(x = n, y = sum_so_far - expected_sum_so_far)) + geom_point()
ggplot(coin_tosses, aes(x=n, y= sum_so_far - expected_sum_so_far))+ geom_point()
set.seed(132) ### Change this seed several times to see what happens
new_tosses = sample(coin_outcomes, size = 10000, replace=TRUE)
new_coin_tosses <- data.frame(n=1:10000, expected_sum_so_far = 1:10000/2,
sum_so_far = cumsum(new_tosses), mean_so_far = cumsum(new_tosses)/(1:10000), expected_mean_so_far = 0.5)
ggplot(new_coin_tosses, aes(x=n, y= sum_so_far - expected_sum_so_far))+ geom_point()
ggplot(new_coin_tosses, aes(x=n, y= mean_so_far - expected_mean_so_far))+ geom_point()
### Repeat exercise D here, but use new_coin_tosses as the dataset
### Repeat exercise E here, but use new_coin_tosses as the dataset
### See how the plots change
# How many search repos did your search find? (Hint: check the list names to
# find an appropriate value).
print(graphics_repos$total_count)
# Exercise 1: reading and querying a web API
# Load the httr and jsonlite libraries for accessing data
# You can also load `dplyr` if you wish to use it
library("httr")
library("jsonlite")
library("dplyr")
# Create a variable base_uri that stores the base URI (as a string) for the
# Github API (https://api.github.com)
base_uri <- "https://api.github.com"
# Under the "Repositories" category of the API documentation, find the endpoint
# that will list _repos in an organization_. Then create a variable named
# `org_resource` that stores the endpoint for the `programming-for-data-science`
# organization repos (this is the _path_ to the resource of interest).
org_resource <- "/orgs/programming-for-data-science/repos"
# Send a GET request to this endpoint (the `base_uri` followed by the
# `org_resource` path). Print the response to show that your request worked.
# (The listed URI will also allow you to inspect the JSON in the browser easily).
response <- GET(paste0(base_uri, org_resource))
print(response)
# Extract the content of the response using the `content()` function, saving it
# in a variable.
response_text <- content(response, "text")
# Convert the content variable from a JSON string into a data frame.
org_repos <- fromJSON(response_text)
# How many (public) repositories does the organization have?
print(nrow(org_repos))
# Now a second query:
# Create a variable `search_endpoint` that stores the endpoint used to search
# for repositories. (Hint: look for a "Search" endpoint in the documentation).
search_endpoint <- "/search/repositories"
# Search queries require a query parameter (for what to search for). Create a
# `query_params` list variable that specifies an appropriate key and value for
# the search term
query_params <- list(q = "graphics")
# Send a GET request to the `search_endpoint`--including your params list as the
# `query`. Print the response to show that your request worked.
response <- GET(paste0(base_uri, search_endpoint), query = query_params)
print(response)
# Extract the content of the response and convert it from a JSON string into a
# data frame.
response_text <- content(response, "text")
graphics_repos = fromJSON(response_text)
# How many search repos did your search find? (Hint: check the list names to
# find an appropriate value).
print(graphics_repos$total_count)
# What are the full names of the top 5 repos in the search results?
graphics_repo_names <- graphics_repos$items$full_name[1:5]
print(graphics_repo_names)
# Exercise 5: dplyr grouped operations
# Install the `"nycflights13"` package. Load (`library()`) the package.
# You'll also need to load `dplyr`
# install.packages("nycflights13")  # should be done already
library("nycflights13")
library("dplyr")
# What was the average departure delay in each month?
# Save this as a data frame `dep_delay_by_month`
# Hint: you'll have to perform a grouping operation then summarizing your data
dep_delay_by_month <- flights %>%
group_by(month) %>%
summarize(delay = mean(dep_delay, na.rm = TRUE))
dep_delay_by_month
# Which month had the greatest average departure delay?
filter(dep_delay_by_month, delay == max(delay)) %>% select(month)
# If your above data frame contains just two columns (e.g., "month", and "delay"
# in that order), you can create a scatterplot by passing that data frame to the
# `plot()` function
plot(dep_delay_by_month)
# To which destinations were the average arrival delays the highest?
# Hint: you'll have to perform a grouping operation then summarize your data
# You can use the `head()` function to view just the first few rows
arr_delay_by_month <- flights %>%
group_by(dest) %>%
summarise(delay = mean(arr_delay, na.rm = TRUE)) %>%
arrange(-delay)
head(arr_delay_by_month)
# You can look up these airports in the `airports` data frame!
filter(airports, faa == arr_delay_by_month$dest[1]) # for example
# Which city was flown to with the highest average speed?
city_fasted_speed <- flights %>%
mutate(speed = distance / air_time * 60) %>%
group_by(dest) %>%
summarise(avg_speed = mean(speed, na.rm = TRUE)) %>%
filter(avg_speed == max(avg_speed, na.rm = TRUE))
city_fasted_speed
# Exercise 6: dplyr join operations
# Install the `"nycflights13"` package. Load (`library()`) the package.
# You'll also need to load `dplyr`
# install.packages("nycflights13")  # should be done already
library("nycflights13")
library("dplyr")
# Create a dataframe of the average arrival delays for each _destination_, then
# use `left_join()` to join on the "airports" dataframe, which has the airport
# information
# Which airport had the largest average arrival delay?
largest_arrival_delay <- flights %>%
group_by(dest) %>%
summarise(avg_delay = mean(arr_delay, na.rm = TRUE)) %>%
mutate(faa = dest) %>%
left_join(airports, by = "faa") %>%
filter(avg_delay == max(avg_delay, na.rm = TRUE))
largest_arrival_delay
# Create a dataframe of the average arrival delay for each _airline_, then use
# `left_join()` to join on the "airlines" dataframe
# Which airline had the smallest average arrival delay?
smallest_airline_delay <- flights %>%
group_by(carrier) %>%
summarise(avg_delay = mean(arr_delay, na.rm = TRUE)) %>%
left_join(airlines, by = "carrier") %>%
filter(avg_delay == max(avg_delay, na.rm = TRUE))
smallest_airline_delay
knitr::opts_chunk$set(echo = TRUE)
source("C18 E1.R")
# load relevant libraries
library("httr")
library("jsonlite")
# Be sure and check the README.md for complete instructions!
# Use `source()` to load your API key variable from the `apikey.R` file you made.
# Make sure you've set your working directory!
source("apikey.R")
# Create a variable `movie_name` that is the name of a movie of your choice.
movie_name <- "Star Wars"
# Construct an HTTP request to search for reviews for the given movie.
# The base URI is `https://api.nytimes.com/svc/movies/v2/`
# The resource is `reviews/search.json`
# See the interactive console for parameter details:
#   https://developer.nytimes.com/movie_reviews_v2.json
#
# You should use YOUR api key (as the `api-key` parameter)
# and your `movie_name` variable as the search query!
base_uri <- "https://api.nytimes.com/svc/movies/v2"
resource <- "/reviews/search.json"
query_params <- list("api-key" = nyt_apikey, query = movie_name)
# Send the HTTP Request to download the data
# Extract the content and convert it from JSON
response <- GET(paste0(base_uri, resource), query = query_params)
body <- fromJSON(content(response, "text"))
# What kind of data structure did this produce? A data frame? A list?
class(body)  # list
is.data.frame(body)  # FALSE
is.list(body)  # TRUE
# Manually inspect the returned data and identify the content of interest
# (which are the movie reviews).
# Use functions such as `names()`, `str()`, etc.
names(body)
names(body$results)
# Flatten the movie reviews content into a data structure called `reviews`
reviews <- flatten(body$results)
# From the most recent review, store the headline, short summary, and link to
# the full article, each in their own variables
first_review <- reviews[1, ]
headline <- first_review$headline
summary <- first_review$summary_short
link <- first_review$link.url
# Create a list of the three pieces of information from above.
# Print out the list.
review <- list(headline=headline, summary=summary, link=link)
setwd("C:/Users/33786/info201/Exercise week 6")
# Use `source()` to load your API key variable from the `apikey.R` file you made.
# Make sure you've set your working directory!
source("apikey.R")
# load relevant libraries
library("httr")
library("jsonlite")
# Be sure and check the README.md for complete instructions!
# Use `source()` to load your API key variable from the `apikey.R` file you made.
# Make sure you've set your working directory!
source("apikey.R")
# Create a variable `movie_name` that is the name of a movie of your choice.
movie_name <- "Star Wars"
# Construct an HTTP request to search for reviews for the given movie.
# The base URI is `https://api.nytimes.com/svc/movies/v2/`
# The resource is `reviews/search.json`
# See the interactive console for parameter details:
#   https://developer.nytimes.com/movie_reviews_v2.json
#
# You should use YOUR api key (as the `api-key` parameter)
# and your `movie_name` variable as the search query!
base_uri <- "https://api.nytimes.com/svc/movies/v2"
resource <- "/reviews/search.json"
query_params <- list("api-key" = nyt_apikey, query = movie_name)
# Send the HTTP Request to download the data
# Extract the content and convert it from JSON
response <- GET(paste0(base_uri, resource), query = query_params)
body <- fromJSON(content(response, "text"))
# What kind of data structure did this produce? A data frame? A list?
class(body)  # list
is.data.frame(body)  # FALSE
is.list(body)  # TRUE
# Manually inspect the returned data and identify the content of interest
# (which are the movie reviews).
# Use functions such as `names()`, `str()`, etc.
names(body)
names(body$results)
# Flatten the movie reviews content into a data structure called `reviews`
reviews <- flatten(body$results)
# From the most recent review, store the headline, short summary, and link to
# the full article, each in their own variables
first_review <- reviews[1, ]
headline <- first_review$headline
summary <- first_review$summary_short
link <- first_review$link.url
# Create a list of the three pieces of information from above.
# Print out the list.
review <- list(headline=headline, summary=summary, link=link)
knitr::opts_chunk$set(echo = TRUE)
source("exercise.R")
headline <- review$headline
knitr::opts_chunk$set(echo = TRUE)
source("exercise.R")
headline <- review$headline
summary <- review$summary
link <- review$link
# Exercise 1: ggplot2 basics
# Install and load the `ggplot2` package
# You will also want to load `dplyr`
install.packages("ggplot2")
library("ggplot2")
library("dplyr")
# For this exercise you'll be working with the `diamonds` data set included in
# the ggplot2 library
# Use `?diamonds` to get more information about this data set (including the
# column descriptions. Also check the _column names_ and the _number of rows_
# in the data set
?diamonds
colnames(diamonds)
nrow(diamonds)
# This data set has A LOT of rows. To make things a bit more readable,
# use dplyr's `sample_n()` function to get a random 1000 rows from the data set
# Store this sample in a variable `diamonds_sample`
diamonds_sample <- sample_n(diamonds, 1000)
# Start by making a new `ggplot` with the `diamonds_sample` as the data (no
# geometry yet)
# What do you see? (What did you expect?)
ggplot(data = diamonds_sample)
# Draw a scatter plot (with point geometry) with for the `diamonds_sample` set,
# with the `carat` mapped to the x-position and `price` mapped to the y-position.
ggplot(data = diamonds_sample) +
geom_point(mapping = aes(x = carat, y = price))
# Draw the same plot as above, but color each of the points based on their
# clarity.
ggplot(data = diamonds_sample) +
geom_point(mapping = aes(x = carat, y = price, color = clarity))
# Draw the same plot as above, but for the entire `diamonds` data set. Note this
# may take a few seconds to generate.
ggplot(data = diamonds) +
geom_point(mapping = aes(x = carat, y = price, color = clarity))
# Draw another scatter plot for `diamonds_sample` of price (y) by carat (x),
# but with all of the dots colored "blue".
# Hint: you'll need to set the color channel, not map a value to it!
ggplot(data = diamonds_sample) +
geom_point(mapping = aes(x = carat, y = price), color = "blue")
# Draw a scatter plot for `diamonds_sample` of `price` by `carat`, where each
# point has an aesthetic _shape_ based on the diamond's `cut`.
ggplot(data = diamonds_sample) +
geom_point(mapping = aes(x = carat, y = price, shape = cut))
# Draw a scatter plot for `diamonds_sample` of *`cut`* by `carat`, where each
# point has an aesthetic _size_ based on the diamond's *`price`*
ggplot(data = diamonds_sample) +
geom_point(mapping = aes(x = carat, y = cut, size = price))
# Try coloring the above plot based on the diamond's price!
ggplot(data = diamonds_sample) +
geom_point(mapping = aes(x = carat, y = cut, size = price, color = price))
# Draw a line plot (with line geometry) for `diamonds_sample`. The x-position
# should be mapped to carat, y-position to price, and color to cut.
ggplot(data = diamonds_sample) +
geom_line(mapping = aes(x = carat, y = price, color = cut))
# That's kind of messy. Try using `smooth` geometry instead.
ggplot(data = diamonds_sample) +
geom_smooth(mapping = aes(x = carat, y = price, color = cut))
# Draw a plot with column geometry (a bar chart), mapping the diamond's `cut` to
# the x-axis and `price` to the y-axis. Note that by default, column geometry
# will us the "sum" of all of the y-values, so that the chart is actually of the
# TOTAL value of all of the diamonds of that cut!
ggplot(data = diamonds_sample) +
geom_col(mapping = aes(x = cut, y = price))
# Add an aesthetic property that will _fill_ each bar geometry based on the
# `clarity` of the diamonds.
# What kind of chart do you get?
ggplot(data = diamonds_sample) +
geom_col(mapping = aes(x = cut, y = price, fill = clarity))
# Draw a plot of the `diamonds_sample` data (price by carat), with both points
# for each diamond AND smoothed lines for each cut (hint: in a separate color)
# Give the points an `alpha` (transparency) of 0.3 to make the plot look nicer
ggplot(data = diamonds_sample) +
geom_point(mapping = aes(x = carat, y = price, color = cut), alpha = 0.3) +
geom_smooth(mapping = aes(x = carat, y = price, color = cut), se = FALSE)
## Bonus
# Draw a column chart of average diamond prices by clarity, and include
# "error bars" marking the standard error of each measurement.
#
# You can calculate standard error as the _standard deviation_ divided by the
# square root of the number of measurements (prices)
# Start by creating a data frame `clarity_summary` that includes summarized data
# for each clarity group. Your summary data should include the mean price and the
# standard error of the price.
clarity_summary <- diamonds %>%
group_by(clarity) %>%
summarize(mean = mean(price), sd = sd(price), se = sd / sqrt(length(price)))
# Then draw the plot. The error bars should stretch from the mean-error to the
# mean+error.
ggplot(data = clarity_summary, mapping = aes(x = clarity, y = mean)) +
geom_bar(mapping = aes(fill = clarity), stat = "identity") +
geom_errorbar(mapping = aes(ymin = (mean - se), ymax = (mean + se)))
# Exercise 2: advanced ggplot2 practice
# Install and load the `ggplot2` package
# install.packages('ggplot2')
library("ggplot2")
# For this exercise you will again be working with the `diamonds` data set.
# Use `?diamonds` to review details about this data set
?diamonds
## Position Adjustments
# Draw a column (bar) chart of diamonds cuts by price, with each bar filled by
# clarity. You should see a _stacked_ bar chart.
ggplot(data = diamonds) +
geom_col(mapping = aes(x = cut, y = price, fill = clarity))
# Draw the same chart again, but with each element positioned to "fill" the y axis
ggplot(data = diamonds) +
geom_col(mapping = aes(x = cut, y = price, fill = clarity), position = "fill")
# Draw the same chart again, but with each element positioned to "dodge" each other
ggplot(data = diamonds) +
geom_col(mapping = aes(x = cut, y = price, fill = clarity), position = "dodge")
# Draw a plot with point geometry with the x-position mapped to `cut` and the
# y-position mapped to `clarity`
# This creates a "grid" grouping the points
ggplot(data = diamonds) +
geom_point(mapping = aes(x = cut, y = clarity))
# Use the "jitter" position adjustment to keep the points from all overlapping!
# (This works a little better with a sample of diamond data, such as from the
# previous exercise).
ggplot(data = diamonds) +
geom_point(mapping = aes(x = cut, y = clarity), position = "jitter")
## Scales
# Draw a "boxplot" (with `geom_boxplot`) for the diamond's price (y) by color (x)
ggplot(data = diamonds) +
geom_boxplot(mapping = aes(x = color, y = price))
# This has a lot of outliers, making it harder to read. To fix this, draw the
# same plot but with a _logarithmic_ scale for the y axis.
ggplot(data = diamonds) +
geom_boxplot(mapping = aes(x = color, y = price)) +
scale_y_log10()
# For another version, draw the same plot but with `violin` geometry instead of
# `boxplot` geometry!
# How does the logarithmic scale change the data presentation?
ggplot(data = diamonds) +
geom_violin(mapping = aes(x = color, y = price)) +
scale_y_log10()
# Another interesting plot: draw a plot of the diamonds price (y) by carat (x),
# using a heatmap of 2d bins (geom_bin2d)
# What happens when you make the x and y channels scale logarithmically?
ggplot(data = diamonds) +
geom_bin2d(mapping = aes(x = carat, y = price)) +
scale_x_log10() +
scale_y_log10()
# Draw a scatter plot for the diamonds price (y) by carat (x). Color each point
# by the clarity (Remember, this will take a while. Use a sample of the diamonds
# for faster results)
ggplot(data = diamonds) +
geom_point(mapping = aes(x = carat, y = price, color = clarity))
# Change the color of the previous plot using a ColorBrewer scale of your choice.
# What looks nice?
ggplot(data = diamonds) +
geom_point(mapping = aes(x = carat, y = price, color = clarity)) +
scale_color_brewer(palette = "Spectral")
## Coordinate Systems
# Draw a bar chart with x-position and fill color BOTH mapped to cut
# For best results, SET the `width` of the geometry to be 1 (fill plot, no space
# between)
# TIP: You can save the plot to a variable for easier modifications
bar <- ggplot(data = diamonds) +
geom_bar(mapping = aes(x = cut, fill = cut), width = 1)
bar
# Draw the same chart, but with the coordinate system flipped
bar + coord_flip()
# Draw the same chart, but in a polar coordinate system. It's a Coxcomb chart!
bar + coord_polar()
## Facets
# Take the scatter plot of price by carat data (colored by clarity) and add
# _facets_ based on the diamond's `color`
ggplot(data = diamonds) +
geom_point(mapping = aes(x = carat, y = price, color = clarity)) +
scale_color_brewer(palette = "Spectral") +
facet_wrap(~ color)
## Saving Plots
# Use the `ggsave()` function to save the current (recent) plot to disk.
# Name the output file "my-plot.png".
# Make sure you've set the working directory!!
ggsave("my-plot.png")
